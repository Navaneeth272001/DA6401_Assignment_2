{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "664774e8",
   "metadata": {},
   "source": [
    "### Question 1 (5 Marks)\n",
    "In most DL applications, instead of training a model from scratch, you would use a model pre-trained on a similar/related task/dataset. From ```torchvision```, you can load **ANY ONE** [model](https://pytorch.org/vision/stable/models.html) (```GoogLeNet```, ```InceptionV3```, ```ResNet50```, ```VGG```, ```EfficientNetV2```, ```VisionTransformer``` etc.) pre-trained  on the ImageNet dataset. Given that ImageNet also contains many animal images, it stands to reason that using a model pre-trained on ImageNet maybe helpful for this task. \n",
    "\n",
    "You will load a pre-trained model and then fine-tune it using the naturalist data that you used in the previous question. Simply put, instead of randomly initialising the weigths of a network you will use the weights resulting from training the model on the ImageNet data (```torchvision``` directly provides these weights). Please answer the following questions:\n",
    "\n",
    "- The dimensions of the images in your data may not be the same as that in the ImageNet data. How will you address this?\n",
    "- ImageNet has $1000$ classes and hence the last layer of the pre-trained model would have $1000$ nodes. However, the naturalist dataset has only $10$ classes. How will you address this?\n",
    "\n",
    "(Note: This question is only to check the implementation. The subsequent questions will talk about how exactly you will do the fine-tuning.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd410f32",
   "metadata": {},
   "source": [
    "### Question 2 (5 Marks)\n",
    "You will notice that ```GoogLeNet```, ```InceptionV3```, ```ResNet50```, ```VGG```, ```EfficientNetV2```, ```VisionTransformer``` are very huge models as compared to the simple model that you implemented in Part A. Even fine-tuning on a small training data may be very expensive. What is a common trick used to keep the training tractable (you will have to read up a bit on this)? Try different variants of this trick and fine-tune the model using the iNaturalist dataset. For example, '_______'ing all layers except the last layer, '_______'ing upto $k$ layers and  '_______'ing the rest. Read up on pre-training and fine-tuning to understand what exactly these terms mean.\n",
    "\n",
    "Write down the at least $3$ different strategies that you tried (simple bullet points would be fine).\n",
    "\n",
    "Answer:\n",
    "In order to reduce the cost of training, we can use the following three strategies:\n",
    "- Feature Extraction: We can freeze all the layers except the last layer, so that we can preserve all the low level features and reduce the memory usage and for quick training\n",
    "- Partial Finetuning: We can freeze the network parameters except for the last \"k\" layers, so that we can fine-tune much deeper. This is improve model performance on the current dataset. We can do this if we have enough compute to train efficiently. \n",
    "- Full Finetuning with discriminative learning rate: We can fine-tune the full network if we have enough compute. while doing this, we can have the learning rate of the early layers of the pretrained network lower, and have slightly larger learning rates for the last few layers. This way, the updation of the parameters in the early layers will happen slowly, kind of slowing down the \"unlearning\" of the trained parameters. But, I guess, this would be a bit expensive strategy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89659afa",
   "metadata": {},
   "source": [
    "### Question 3 (10 Marks)\n",
    "Now fine-tune the model using **ANY ONE** of the listed strategies that you discussed above. Based on these experiments write down some insightful inferences comparing training from scratch and fine-tuning a large pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bd45f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnavaneeth001\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/e_disk/DA24S008/DA6401_Assignment_2/partB/wandb/run-20250418_213805-zj02v67f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/navaneeth001/Assignment2/runs/zj02v67f' target=\"_blank\">googlenet_frozen_early_layers</a></strong> to <a href='https://wandb.ai/navaneeth001/Assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/navaneeth001/Assignment2' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/navaneeth001/Assignment2/runs/zj02v67f' target=\"_blank\">https://wandb.ai/navaneeth001/Assignment2/runs/zj02v67f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 BasicConv2d(\n",
      "  (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "1 MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "2 BasicConv2d(\n",
      "  (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "3 BasicConv2d(\n",
      "  (conv): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "4 MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "5 Inception(\n",
      "  (branch1): BasicConv2d(\n",
      "    (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (branch2): Sequential(\n",
      "    (0): BasicConv2d(\n",
      "      (conv): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (branch3): Sequential(\n",
      "    (0): BasicConv2d(\n",
      "      (conv): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (branch4): Sequential(\n",
      "    (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "6 Inception(\n",
      "  (branch1): BasicConv2d(\n",
      "    (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (branch2): Sequential(\n",
      "    (0): BasicConv2d(\n",
      "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (branch3): Sequential(\n",
      "    (0): BasicConv2d(\n",
      "      (conv): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (branch4): Sequential(\n",
      "    (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "7 MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "8 Inception(\n",
      "  (branch1): BasicConv2d(\n",
      "    (conv): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (branch2): Sequential(\n",
      "    (0): BasicConv2d(\n",
      "      (conv): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(208, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (branch3): Sequential(\n",
      "    (0): BasicConv2d(\n",
      "      (conv): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (branch4): Sequential(\n",
      "    (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "9 Inception(\n",
      "  (branch1): BasicConv2d(\n",
      "    (conv): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (branch2): Sequential(\n",
      "    (0): BasicConv2d(\n",
      "      (conv): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(224, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (branch3): Sequential(\n",
      "    (0): BasicConv2d(\n",
      "      (conv): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (branch4): Sequential(\n",
      "    (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "10 Inception(\n",
      "  (branch1): BasicConv2d(\n",
      "    (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (branch2): Sequential(\n",
      "    (0): BasicConv2d(\n",
      "      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (branch3): Sequential(\n",
      "    (0): BasicConv2d(\n",
      "      (conv): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (branch4): Sequential(\n",
      "    (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "11 Inception(\n",
      "  (branch1): BasicConv2d(\n",
      "    (conv): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (branch2): Sequential(\n",
      "    (0): BasicConv2d(\n",
      "      (conv): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(288, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (branch3): Sequential(\n",
      "    (0): BasicConv2d(\n",
      "      (conv): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (branch4): Sequential(\n",
      "    (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "12 Inception(\n",
      "  (branch1): BasicConv2d(\n",
      "    (conv): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (branch2): Sequential(\n",
      "    (0): BasicConv2d(\n",
      "      (conv): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (branch3): Sequential(\n",
      "    (0): BasicConv2d(\n",
      "      (conv): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (branch4): Sequential(\n",
      "    (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "13 MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "14 Inception(\n",
      "  (branch1): BasicConv2d(\n",
      "    (conv): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (branch2): Sequential(\n",
      "    (0): BasicConv2d(\n",
      "      (conv): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (branch3): Sequential(\n",
      "    (0): BasicConv2d(\n",
      "      (conv): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (branch4): Sequential(\n",
      "    (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "15 Inception(\n",
      "  (branch1): BasicConv2d(\n",
      "    (conv): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (branch2): Sequential(\n",
      "    (0): BasicConv2d(\n",
      "      (conv): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (branch3): Sequential(\n",
      "    (0): BasicConv2d(\n",
      "      (conv): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(48, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (branch4): Sequential(\n",
      "    (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=True)\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "16 AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "17 Dropout(p=0.2, inplace=False)\n",
      "18 Linear(in_features=1024, out_features=1000, bias=True)\n",
      "[Epoch 1/10] Train Loss: 1.0321, Train Acc: 0.6769 | Val Loss: 0.7460, Val Acc: 0.7568\n",
      "[Epoch 2/10] Train Loss: 0.6534, Train Acc: 0.7885 | Val Loss: 0.6881, Val Acc: 0.7766\n",
      "[Epoch 3/10] Train Loss: 0.5041, Train Acc: 0.8407 | Val Loss: 0.6637, Val Acc: 0.7771\n",
      "[Epoch 4/10] Train Loss: 0.3836, Train Acc: 0.8811 | Val Loss: 0.6801, Val Acc: 0.7837\n",
      "[Epoch 5/10] Train Loss: 0.2685, Train Acc: 0.9203 | Val Loss: 0.7329, Val Acc: 0.7755\n",
      "[Epoch 6/10] Train Loss: 0.1928, Train Acc: 0.9460 | Val Loss: 0.7809, Val Acc: 0.7733\n",
      "[Epoch 7/10] Train Loss: 0.1365, Train Acc: 0.9663 | Val Loss: 0.8051, Val Acc: 0.7793\n",
      "[Epoch 8/10] Train Loss: 0.0903, Train Acc: 0.9790 | Val Loss: 0.8654, Val Acc: 0.7711\n",
      "[Epoch 9/10] Train Loss: 0.0835, Train Acc: 0.9801 | Val Loss: 0.9366, Val Acc: 0.7656\n",
      "[Epoch 10/10] Train Loss: 0.0582, Train Acc: 0.9875 | Val Loss: 0.9121, Val Acc: 0.7733\n",
      "\n",
      " Test Loss: 1.0103 | Test Accuracy: 0.7340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "wandb.init(project='assignment_2', name='googlenet_frozen_early_layers')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "data = '/mnt/e_disk/DA24S008/DA6401_Assignment_2/dataset/inaturalist_12K'\n",
    "trainset = ImageFolder(root=os.path.join(data, 'train'), transform=transform)\n",
    "testset = ImageFolder(root=os.path.join(data, 'val'), transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "train_subset, val_subset = random_split(trainset, [train_size, val_size])\n",
    "\n",
    "batch_size = 64\n",
    "trainloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "valloader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "model = models.googlenet(weights=models.GoogLeNet_Weights.IMAGENET1K_V1)\n",
    "\n",
    "for idx, child in enumerate(model.children()):\n",
    "    print(idx, child)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.inception5b.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model.fc = nn.Linear(model.fc.in_features, len(trainset.classes))\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0005)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_loss /= val_total\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    wandb.log({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': train_loss,\n",
    "        'train_acc': train_acc,\n",
    "        'val_loss': val_loss,\n",
    "        'val_acc': val_acc\n",
    "    })\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"googlenet_finetuned.pth\")\n",
    "wandb.save(\"googlenet_finetuned.pth\")\n",
    "\n",
    "model.eval()\n",
    "test_loss, test_correct, test_total = 0.0, 0, 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in testloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "test_loss /= test_total\n",
    "test_acc = test_correct / test_total\n",
    "\n",
    "print(f\"\\n Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
    "wandb.log({'test_loss': test_loss, 'test_acc': test_acc})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da24s008",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
