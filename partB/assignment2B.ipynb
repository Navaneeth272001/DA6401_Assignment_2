{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "664774e8",
   "metadata": {},
   "source": [
    "### Question 1 (5 Marks)\n",
    "In most DL applications, instead of training a model from scratch, you would use a model pre-trained on a similar/related task/dataset. From ```torchvision```, you can load **ANY ONE** [model](https://pytorch.org/vision/stable/models.html) (```GoogLeNet```, ```InceptionV3```, ```ResNet50```, ```VGG```, ```EfficientNetV2```, ```VisionTransformer``` etc.) pre-trained  on the ImageNet dataset. Given that ImageNet also contains many animal images, it stands to reason that using a model pre-trained on ImageNet maybe helpful for this task. \n",
    "\n",
    "You will load a pre-trained model and then fine-tune it using the naturalist data that you used in the previous question. Simply put, instead of randomly initialising the weigths of a network you will use the weights resulting from training the model on the ImageNet data (```torchvision``` directly provides these weights). Please answer the following questions:\n",
    "\n",
    "- The dimensions of the images in your data may not be the same as that in the ImageNet data. How will you address this?\n",
    "- ImageNet has $1000$ classes and hence the last layer of the pre-trained model would have $1000$ nodes. However, the naturalist dataset has only $10$ classes. How will you address this?\n",
    "\n",
    "(Note: This question is only to check the implementation. The subsequent questions will talk about how exactly you will do the fine-tuning.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3e85423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the part A question, I have done preprocessing of the inaturalist dataset, by resizing the images to 256 X 256.. using the transform module of torchvision package.\n",
    "#Based on few refereces, there is also an option to crop the centre of the image for usage.\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torchvision as tv\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "model = models.googlenet(pretrained=True)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "#To match the number of classes with the last layer nodes, I have replaced the final fully connected layer with a new layer that has only 10 neurons.\n",
    "\n",
    "num_classes = 10\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd410f32",
   "metadata": {},
   "source": [
    "### Question 2 (5 Marks)\n",
    "You will notice that ```GoogLeNet```, ```InceptionV3```, ```ResNet50```, ```VGG```, ```EfficientNetV2```, ```VisionTransformer``` are very huge models as compared to the simple model that you implemented in Part A. Even fine-tuning on a small training data may be very expensive. What is a common trick used to keep the training tractable (you will have to read up a bit on this)? Try different variants of this trick and fine-tune the model using the iNaturalist dataset. For example, '_______'ing all layers except the last layer, '_______'ing upto $k$ layers and  '_______'ing the rest. Read up on pre-training and fine-tuning to understand what exactly these terms mean.\n",
    "\n",
    "Write down the at least $3$ different strategies that you tried (simple bullet points would be fine).\n",
    "\n",
    "Answer:\n",
    "In order to reduce the cost of training, we can use the following three strategies:\n",
    "- Feature Extraction: We can freeze all the layers except the last layer, so that we can preserve all the low level features and reduce the memory usage and for quick training\n",
    "- Partial Finetuning: We can freeze the network parameters except for the last \"k\" layers, so that we can fine-tune much deeper. This is improve model performance on the current dataset. We can do this if we have enough compute to train efficiently. \n",
    "- Full Finetuning with discriminative learning rate: We can fine-tune the full network if we have enough compute. while doing this, we can have the learning rate of the early layers of the pretrained network lower, and have slightly larger learning rates for the last few layers. This way, the updation of the parameters in the early layers will happen slowly, kind of slowing down the \"unlearning\" of the trained parameters. But, I guess, this would be a bit expensive strategy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89659afa",
   "metadata": {},
   "source": [
    "### Question 3 (10 Marks)\n",
    "Now fine-tune the model using **ANY ONE** of the listed strategies that you discussed above. Based on these experiments write down some insightful inferences comparing training from scratch and fine-tuning a large pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bd45f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da24s008",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
